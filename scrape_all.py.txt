import requests
import json
import os
import time
from bs4 import BeautifulSoup
from datetime import datetime

HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'}

os.makedirs("data", exist_ok=True)

def scrape_airdrops():
    """Multi-source: airdrops.io + CoinGecko (backup)"""
    data = []
    
    # Source 1: airdrops.io (flexible)
    try:
        r = requests.get("https://airdrops.io/latest/", headers=HEADERS, timeout=15)
        soup = BeautifulSoup(r.text, 'html.parser')
        items = soup.select('article, .airdrop-item, [class*="drop"]')[:5]
        for i in items:
            title_elem = i.select_one('h2 a, h3 a, .title a')
            if title_elem:
                title = title_elem.text.strip()
                link = title_elem.get('href', '')
                if link.startswith('/'): link = "https://airdrops.io" + link
                reward = i.select_one('.reward, .prize').text.strip() if i.select_one('.reward, .prize') else "TBA"
                end = i.select_one('.end, .deadline').text.strip() if i.select_one('.end, .deadline') else "Ongoing"
                data.append({"source": "airdrops.io", "title": title, "link": link, "reward": reward, "end": end})
    except Exception as e:
        print(f"airdrops.io error: {e}")
    
    # Source 2: CoinGecko (lebih reliable)
    try:
        r = requests.get("https://www.coingecko.com/en/airdrop", headers=HEADERS, timeout=15)
        soup = BeautifulSoup(r.text, 'html.parser')
        items = soup.select('tr, .airdrop-row')[:5]
        for row in items:
            cols = row.select('td')
            if len(cols) >= 3:
                title = cols[1].text.strip()
                link = "https://www.coingecko.com" + cols[1].select_one('a')['href'] if cols[1].select_one('a') else ""
                reward = cols[2].text.strip() if len(cols) > 2 else "TBA"
                end = cols[3].text.strip() if len(cols) > 3 else "Ongoing"
                data.append({"source": "coingecko.com", "title": title, "link": link, "reward": reward, "end": end})
    except Exception as e:
        print(f"coingecko error: {e}")
    
    return data[:10]  # Max 10

def scrape_news():
    """CryptoPanic API-like scrape (stable source)"""
    try:
        r = requests.get("https://cryptopanic.com/news/", headers=HEADERS, timeout=15)
        soup = BeautifulSoup(r.text, 'html.parser')
        items = soup.select('.news-item, article')[:8]
        data = []
        for i in items:
            title_elem = i.select_one('h2 a, .title')
            if title_elem:
                title = title_elem.text.strip()
                link = title_elem.get('href', '')
                if link.startswith('/'): link = "https://cryptopanic.com" + link
                date = i.select_one('.date, time').text.strip() if i.select_one('.date, time') else "Today"
                data.append({"title": title, "date": date, "link": link})
        return data
    except Exception as e:
        print(f"news error: {e}")
        # Fallback dummy jika gagal
        return [
            {"title": "Test News: Bitcoin Up 5%", "date": "Today", "link": "https://example.com"},
            {"title": "Airdrop Update 2025", "date": "Nov 15", "link": "https://example.com"}
        ]

# Jalankan & simpan
all_airdrops = scrape_airdrops()
news_items = scrape_news()

with open("data/airdrops.json", 'w', encoding='utf-8') as f:
    json.dump(all_airdrops, f, indent=2, ensure_ascii=False)
with open("data/news.json", 'w', encoding='utf-8') as f:
    json.dump(news_items, f, indent=2, ensure_ascii=False)

print(f"[{datetime.now().strftime('%Y-%m-%d %H:%M EET')}] Updated: {len(all_airdrops)} airdrops | {len(news_items)} news")
